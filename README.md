---
title: TopicBuzz
colorFrom: yellow
colorTo: gray
sdk: streamlit
sdk_version: 1.29.0
app_file: app.py
pinned: false
---

Ignore the above, it's necessary for HF spaces

### LINK TO HF SPACE: https://huggingface.co/spaces/Wyomike/topicBuzz

# TopicBuzz
CS 452 final project focused on analyzing buzz topics

## Class stuff

### Summary of the project (one to three sentences)

This project pulls posts from popular figures of the social media platform Mastodon and embeds their content in vectors to cluster posts into topics. We can form a graph out of these topics and users where topics and users are nodes and posts are edges. Edges are colored in a gradient from red (new) to blue (older). This graph will allow a user to understand what are popular topics at a glance while also being able to observe which topics have overlap/are connected by many people. Posts are read daily and the clustering model BERTopic is run weekly to shift centroids and relabel clusters based on bag-of-words labels.

Currently the project doesn't have usernames but just has Mastodon IDs since I forgot to fetch those and I'm not sure about the ethics of including names. 

### Diagrams, demo video or gif, etc.

Example of a generated graph
<img width="1361" height="815" alt="image" src="https://github.com/user-attachments/assets/f080ff4e-4863-4526-ba94-c3fb600c951b" />

### What did you learn in this project?

I learned a lot about the feasibilitiy of making a project like this for a low price point and how difficult it is to dynamically maintain a graph. Traditional HDBScan doesn't work due to the large volume of topics and the online nature of incorporating new data. So, in an effort to do this all for free, I tried both chromaDB and Supabase for free database providers while also trying both Github Pages and Huggingface Spaces for free hosting. It requires a bit more thinking to understand how to allow the project to run within very low constraints. I had to try to be a bit more efficient in my storage of files and models. I also discovered you have to be pretty careful with accidental revealing of secrets in github.  

### Does your project integrate with AI in any interesting way? If yes, please describe.

The model does use NLP techniques in using vector embeddings of text, as well as generates more natural labels by handing the bag-of-words topic labels generated by BERTopic clustering to a local LLM. 

### How did you use AI to assist in building your project? Please, describe.

I basically came up with ideas and architectures that I wanted to use, then started out basic functionalities by myself, then handed these basic ideas to Gemini to flesh out. As a result I was able to expand my scope much faster and more easily than I would have if I had done this myself. Machine learning bugs are no fun to debug by yourself. 

I also used Gemini to pivot most of my code from chromaDB to Supabase and HF spaces. As a result, I did have a lot of headaches from Gemini missing functionalities as it reconstructed scripts. Because of this, in the future, I think I'd prefer to mostly have Gemini handle pieces of scripts rather than put together pieces I've made and as a result risk dropping functionalities. 

### Discuss why this project is interesting to you! 

I really like data exploration and I've always been curious about how trends and topics spread on media. My key idea here was really to see if "influencers" actually start trends or just echo things, and because I forgot to fetch the names and that'd take forever I haven't figured it out, but once that long api call is done I'm excited to see the results. 

There's also just a lot of really cool potential for graph and connection analysis of Mastodon through this. I'd love to add more features and filters in the future as I have time. It might be interesting to have a separate graph for news organizations and check out how stories tend to spread as that might inform who is influenced by whose writing.

### Key learnings from the project (please try to come up with at least 3)

1. Find compressible ways to store large files since they'll eat up you quotas rapidly
2. Try to match your dev environment to your production environment as closely as you can
3. If you want higher scaling and access to large data it really just isn't feasible for free.

### Explanation if applicable of failover strategy, scaling characteristics, performance characteristics, authentication, concurrency, etc.

Data is backed up on my local computer. Scaling would require paying for higher tiers of Supabase. Huggingface spaces, best I can tell, will handle individual instances for as many visitors as it gets. Other than that, most of the processing is pretty light and independant of outside users. My local computer runs daily and weekly scripts to update the data and push to/prune Supabase data.
